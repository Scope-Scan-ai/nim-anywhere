{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using TAO Grounding DINO\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n",
    "\n",
    "## What is Grounding DINO?\n",
    "\n",
    "[Grounding DINO](https://arxiv.org/abs/2303.05499) is a state of the art open-set object detection model based on DINO. Grounding DINO can detect arbitrary objects with human inputs such as category names or referring expressions. Compared to DINO, Grounding DINO has text encoder and cross attention modules to align the bounding boxes with given categories / phrases.\n",
    "\n",
    "In TAO, only single type of backbone network is supported: [Swin](https://arxiv.org/abs/2103.14030). In this notebook, we use the pretrained Swin-Tiny Grounding DINO and showcase how we can finetune on [Hard Hat Worker](https://public.roboflow.com/object-detection/hard-hat-workers/) dataset for the state of the art mAP result.\n",
    "\n",
    "### Sample prediction of Swin-Tiny + Grounding DINO model\n",
    "<img align=\"center\" src=\"sample.jpg\" width=\"960\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained model and finetune a Grounding DINO model on Hard Hat Worker dataset\n",
    "* Evaluate the trained model\n",
    "\n",
    "For inference and deployment workflow, please refer to zero-shot inference notboook.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of Grounding DINO using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2)\n",
    "3. [Run zero-shot evaluation](#head-3)\n",
    "4. [Provide training specification](#head-4)\n",
    "5. [Run TAO training](#head-5)\n",
    "6. [Evaluate a trained model](#head-6)\n",
    "7. [Visualize inferences](#head-7)\n",
    "8. [Deploy](#head-8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/grounding_dino/results`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workbench/tao-experiments\n",
      "ls: cannot access 'workspace_file': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# !echo ${PWD}\n",
    "# !ls /home/scopescan/workspace/grounding_dino-2/tao-experiments\n",
    "workspace_file = os.path.expanduser(\"~/tao-experiments\")\n",
    "print(workspace_file)\n",
    "!ls workspace_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T15:58:40.654531Z",
     "start_time": "2024-09-25T15:58:40.651022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOCAL_PROJECT_DIR=/home/workbench\n",
      "==============================\n",
      "/home/workbench\n",
      "data  grounding_dino  ngccli\n",
      "---------\n",
      "/project/grounding_dino/specs\n",
      "convert.yaml\t     evaluate.yaml  gen_trt_engine.yaml  train.yaml\n",
      "download_hardhat.sh  export.yaml    infer.yaml\n",
      "---------\n",
      "/home/workbench/data\n",
      "annotations  hardhat.zip  odvg\traw-data  specs\n",
      "---------\n",
      "/home/workbench/grounding_dino/results\n",
      "evaluate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# %env LOCAL_PROJECT_DIR=tao-experiments\n",
    "# %env LOCAL_PROJECT_DIR=/project/grounding_dino/tao-experiments\n",
    "# workspace = os.path.expanduser(\"~\")\n",
    "# print(workspace)\n",
    "%env LOCAL_PROJECT_DIR=/home/workbench\n",
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"grounding_dino\", \"results\")\n",
    "# ----\n",
    "os.environ[\"HOST_IMAGE_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"raw-data\")\n",
    "os.environ[\"HOST_ANNOTATIONS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"annotations\")\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"specs\")\n",
    "os.environ[\"HOST_CONVERT_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"convert\")\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/grounding_dino\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "\n",
    "print('==============================')\n",
    "!echo $LOCAL_PROJECT_DIR\n",
    "!ls $LOCAL_PROJECT_DIR\n",
    "print('---------')\n",
    "!echo $HOST_SPECS_DIR\n",
    "!ls $HOST_SPECS_DIR\n",
    "print('---------')\n",
    "!echo $HOST_DATA_DIR\n",
    "!ls $HOST_DATA_DIR\n",
    "print('---------')\n",
    "!echo $HOST_RESULTS_DIR\n",
    "!ls $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.expanduser(\"~\"))\n",
    "print(local_proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T21:01:47.637861Z",
     "start_time": "2024-09-24T21:01:47.307433Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p $HOST_DATA_DIR\n",
    "!mkdir -p $HOST_SPECS_DIR\n",
    "!mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:22:37.811748Z",
     "start_time": "2024-09-25T16:22:37.806865Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "         # Mapping the Local project directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            # \"source\": os.path.expanduser(\"~\"),\n",
    "            \"destination\": \"/opt/nvidia/tao-experiments\"\n",
    "        },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_IMAGE_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data/raw-data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_ANNOTATIONS_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data/annotations\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"~/.cache\",\n",
    "           \"destination\": \"/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"64G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         },\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid()),\n",
    "        \"network\": \"grounding_dino\",\n",
    "        \"privileged\": True\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:22:37.811748Z",
     "start_time": "2024-09-25T16:22:37.806865Z"
    }
   },
   "outputs": [],
   "source": [
    "# COPY\n",
    "\n",
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "         # Mapping the Local project directory\n",
    "        {\n",
    "            # \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"source\": os.path.expanduser(\"~\"),\n",
    "            \"destination\": \"/opt/nvidia/tao-experiments\"\n",
    "        },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_IMAGE_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data/raw-data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_ANNOTATIONS_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data/annotations\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/data/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/opt/nvidia/tao-experiments/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"~/.cache\",\n",
    "           \"destination\": \"/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"64G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         },\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid()),\n",
    "        \"network\": \"workbench\",\n",
    "        \"privileged\": True\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:22:48.663783Z",
     "start_time": "2024-09-25T16:22:48.552460Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.10. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.7, <=3.10.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "task_group: ['model', 'dataset', 'deploy']\n",
      "format_version: 3.0\n",
      "toolkit_version: 5.5.0\n",
      "published_date: 08/26/2024\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COPY DATA TO CONTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS                                NAMES\n",
      "3aab2ca95355   project-nim-anywhere   \"/entrypoint.sh tail…\"   2 minutes ago    Up 2 minutes    3030/tcp, 7070/tcp                   project-nim-anywhere\n",
      "f85b48b6b8d3   traefik:v2.10.7        \"/entrypoint.sh trae…\"   36 minutes ago   Up 36 minutes   80/tcp, 127.0.0.1:10000->10000/tcp   workbench-proxy\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c979b7186efb7ef6a92def8976c67d0f4cf30e7e3e032e60afc87fad5e0bdef2\n"
     ]
    }
   ],
   "source": [
    "!docker run -d \\\n",
    "    -v /project/grounding_dino/tao-experiments:/opt/nvidia/tao-experiments \\\n",
    "    nvcr.io/nvidia/tao/tao-toolkit:5.5.0-data-services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: IMAGE_DIR=/opt/nvidia/tao-experiments/images\n",
      "env: RESULTS_DIR=/opt/nvidia/tao-experiments/results\n",
      "env: SPECS_DIR=/opt/nvidia/tao-experiments/specs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"grounding_dino\", \"results\")\n",
    "# ----\n",
    "os.environ[\"HOST_IMAGE_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"raw-data\")\n",
    "os.environ[\"HOST_ANNOTATIONS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"annotations\")\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\", \"specs\")\n",
    "os.environ[\"HOST_CONVERT_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"convert\")\n",
    "\n",
    "\n",
    "# SET CONTAINER VARIABILES\n",
    "%env IMAGE_DIR = /opt/nvidia/tao-experiments/images\n",
    "%env RESULTS_DIR = /opt/nvidia/tao-experiments/results\n",
    "%env SPECS_DIR = /opt/nvidia/tao-experiments/specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/grounding_dino/tao-experiments/data/raw-data\n",
      "/project/grounding_dino/tao-experiments/data/specs\n"
     ]
    }
   ],
   "source": [
    "!echo $HOST_IMAGE_DIR\n",
    "!echo $HOST_SPECS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied 3.91MB to 27f246cad250:/opt/nvidia/tao-experiments/data/\n",
      "Successfully copied 4.07MB to 27f246cad250:/opt/nvidia/tao-experiments/data/\n"
     ]
    }
   ],
   "source": [
    "# COPY IMAGES TO CONTAINER\n",
    "# !docker cp $HOST_IMAGE_DIR/ 7b858ab455e1:/opt/nvidia/tao-experiments/data/\n",
    "\n",
    "# COPY SPEC FILES TO CONTAINER\n",
    "# !docker cp $HOST_SPECS_DIR 27f246cad250:/opt/nvidia/tao-experiments/\n",
    "\n",
    "# COPY ANNOTATIONS FILES TO CONTAINER\n",
    "# !docker cp $HOST_ANNOTATIONS_DIR 27f246cad250:/opt/nvidia/tao-experiments/data/\n",
    "\n",
    "# COPY ODVG FILES TO CONTAINER\n",
    "# !docker cp $HOST_DATA_DIR/odvg 27f246cad250:/opt/nvidia/tao-experiments/data/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the Hard Hat Worker dataset for the tutorial. The following script will download HardHat dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:59:00.705076Z",
     "start_time": "2024-09-24T22:58:54.306636Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create local dir\n",
    "# !mkdir -p $HOST_DATA_DIR\n",
    "# # Download the data\n",
    "# !bash $HOST_SPECS_DIR/download_hardhat.sh $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:08:05.636401Z",
     "start_time": "2024-09-25T16:08:05.524295Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Verification\n",
    "# !ls -l $HOST_DATA_DIR/raw-data/train2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:23:12.609406Z",
     "start_time": "2024-09-25T16:23:12.387933Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create ODVG folder\n",
    "# !mkdir -p $HOST_DATA_DIR/odvg\n",
    "# !mkdir -p $HOST_DATA_DIR/odvg/annotations\n",
    "\n",
    "# # NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# # The data is saved here\n",
    "# %env DATA_DIR = /data\n",
    "# %env SPECS_DIR = /specs\n",
    "# %env RESULTS_DIR = /results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:12:36.854263Z",
     "start_time": "2024-09-25T16:12:36.745119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/specs\n",
      "/data\n"
     ]
    }
   ],
   "source": [
    "!echo $SPECS_DIR\n",
    "!echo $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:23:46.277285Z",
     "start_time": "2024-09-25T16:23:27.913729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 16:17:09,867 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2024-11-01 16:17:09,918 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:5.5.0-data-services\n",
      "2024-11-01 16:17:10,005 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 301: Printing tty value True\n",
      "[2024-11-01 16:17:12,601 - TAO Toolkit - matplotlib - WARNING] Matplotlib created a temporary cache directory at /tmp/matplotlib-ouj21u56 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "[2024-11-01 16:17:12,845 - TAO Toolkit - matplotlib.font_manager - INFO] generated new fontManager\n",
      "There was a problem when trying to write in your cache folder (/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/annotations\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_ds/annotations/entrypoint/annotations.py\", line 43, in main\n",
      "    launch(vars(args), unknown_args, subtasks, task=\"annotations\")\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia_tao_ds/core/entrypoint/entrypoint.py\", line 111, in launch\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Experiment spec file wasn not found at /opt/nvidia/inference/specs/convert.yaml\n",
      "2024-11-01 16:17:15,111 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 363: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# Convert COCO to ODVG format required for Grounding DINO\n",
    "!tao dataset annotations convert \\\n",
    "            -e $SPECS_DIR/convert.yaml \\\n",
    "            coco.ann_file=$DATA_DIR/HardHatWorkers/raw/train/annotations_without_background.json \\\n",
    "            results_dir=$DATA_DIR/odvg/annotations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:24:11.043432Z",
     "start_time": "2024-09-25T16:23:53.547060Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Convert COCO validation annotations to have categoy id ranging from 0 to 79.\n",
    "# # This is required for computing validation loss during Grounding DINO training.\n",
    "# !tao dataset annotations convert \\\n",
    "#             -e $SPECS_DIR/convert.yaml \\\n",
    "#             coco.ann_file=$DATA_DIR/HardHatWorkers/raw/valid/annotations_without_background.json \\\n",
    "#             results_dir=$DATA_DIR/odvg/annotations/ \\\n",
    "#             data.output_format=\"COCO\" \\\n",
    "#             coco.use_all_categories=True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download pre-trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:25:45.172020Z",
     "start_time": "2024-09-25T16:25:43.392028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:25:56.706380Z",
     "start_time": "2024-09-25T16:25:54.211324Z"
    }
   },
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tao/grounding_dino:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:28:46.462542Z",
     "start_time": "2024-09-25T16:28:22.801836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/grounding_dino:grounding_dino_swin_tiny_commercial_trainable_v1.0 --dest $LOCAL_PROJECT_DIR/grounding_dino/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:28:52.374415Z",
     "start_time": "2024-09-25T16:28:52.262018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that model is downloaded into dir.\n",
      "total 2022184\n",
      "-rwxrwxrwx 1 workbench workbench       2360 Sep 25 16:28 experiment.yaml\n",
      "-rwxrwxrwx 1 workbench workbench 2070704394 Sep 25 16:28 grounding_dino_swin_tiny_commercial_trainable.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_PROJECT_DIR/grounding_dino/grounding_dino_vgrounding_dino_swin_tiny_commercial_trainable_v1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run zero-shot evaluation <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "Because Grounding DINO is a multi-modal object detector with text encoder, we can run zero-shot evaluation on any dataset using the class category names as input. Let's see the zero-shot mAP of our pretrained Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:34:26.217136Z",
     "start_time": "2024-09-25T16:33:30.785703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-25 11:33:30,961 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\r\n",
      "2024-09-25 11:33:31,011 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:5.5.0-pyt\r\n",
      "2024-09-25 11:33:31,056 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 301: Printing tty value True\r\n",
      "[2024-09-25 16:33:34,770 - TAO Toolkit - root - WARNING] Number of gpus 2 != len([0]).\r\n",
      "[2024-09-25 16:33:34,770 - TAO Toolkit - root - INFO] Using GPUs [0, 1] (total 2)\r\n",
      "sys:1: UserWarning: \r\n",
      "'evaluate.yaml' is validated against ConfigStore schema with the same name.\r\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\r\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\r\n",
      "/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:107: UserWarning: \r\n",
      "'evaluate.yaml' is validated against ConfigStore schema with the same name.\r\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\r\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\r\n",
      "  _run_hydra(\r\n",
      "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\r\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\r\n",
      "  ret = run_job(\r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3553.)\r\n",
      "return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]Evaluate results will be saved at: /results/evaluate\r\n",
      "final text_encoder_type: bert-base-uncased\r\n",
      "\r\n",
      "tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 430kB/s]\r\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 6.60MB/s]\r\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 34.2MB/s]\r\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 5.98MB/s]\r\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:04<00:00, 109MB/s]]final text_encoder_type: bert-base-uncased\r\n",
      "GPU available: True (cuda), used: True\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\r\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\r\n",
      "sys:1: UserWarning: \r\n",
      "'evaluate.yaml' is validated against ConfigStore schema with the same name.\r\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\r\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\r\n",
      "/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:107: UserWarning: \r\n",
      "'evaluate.yaml' is validated against ConfigStore schema with the same name.\r\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\r\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\r\n",
      "  _run_hydra(\r\n",
      "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\r\n",
      "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\r\n",
      "  ret = run_job(\r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3553.)\r\n",
      "return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]Evaluate results will be saved at: /results/evaluate\r\n",
      "final text_encoder_type: bert-base-uncased\r\n",
      "final text_encoder_type: bert-base-uncased\r\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\r\n",
      "Missing logger folder: /results/evaluate/lightning_logs\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "distributed_backend=nccl\r\n",
      "All distributed processes registered. Starting with 2 processes\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "Missing logger folder: /results/evaluate/lightning_logs\r\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:232: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\r\n",
      "\r\n",
      "Testing DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:993: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.  warnings.warn(\r\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:993: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n",
      "  warnings.warn(\r\n",
      "\r\n",
      "Testing DataLoader 0:  92%|█████████▏| 12/13 [00:10<00:00,  1.16it/s]/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/distributed/comm.py:65: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  storage = torch.ByteStorage.from_buffer(buffer)\r\n",
      "\r\n",
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:10<00:00,  1.22it/s]/usr/local/lib/python3.10/dist-packages/nvidia_tao_pytorch/core/distributed/comm.py:65: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()  storage = torch.ByteStorage.from_buffer(buffer)\r\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_mAP', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\r\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_mAP50', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.821\r\n",
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:10<00:00,  1.18it/s]┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\r\n",
      "┃        Test metric        ┃       DataLoader 0        ┃\r\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\r\n",
      "│         test_mAP          │    0.13928461074829102    │\r\n",
      "│        test_mAP50         │    0.24138285219669342    │\r\n",
      "└───────────────────────────┴───────────────────────────┘\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.821[2024-09-25 16:34:14,976 - TAO Toolkit - root - INFO] Sending telemetry data.\r\n",
      "[2024-09-25 16:34:14,976 - TAO Toolkit - root - INFO] ================> Start Reporting Telemetry <================\r\n",
      "[2024-09-25 16:34:14,976 - TAO Toolkit - root - INFO] Sending {'version': '5.5.0', 'action': 'evaluate', 'network': 'grounding_dino', 'gpu': ['NVIDIA-GeForce-RTX-3090', 'NVIDIA-GeForce-RTX-3090'], 'success': True, 'time_lapsed': 39} to https://api.tao.ngc.nvidia.com.\r\n",
      "[2024-09-25 16:34:15,303 - TAO Toolkit - root - INFO] Telemetry sent successfully.\r\n",
      "[2024-09-25 16:34:15,304 - TAO Toolkit - root - INFO] ================> End Reporting Telemetry <================\r\n",
      "[2024-09-25 16:34:15,304 - TAO Toolkit - root - INFO] Execution status: PASS\r\n",
      "2024-09-25 11:34:15,973 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 363: Stopping container.\r\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot evaluation\n",
    "!tao model grounding_dino evaluate \\\n",
    "            -e $SPECS_DIR/evaluate.yaml \\\n",
    "            evaluate.checkpoint=/workspace/tao-experiments/grounding_dino/grounding_dino_vgrounding_dino_swin_tiny_commercial_trainable_v1.0/grounding_dino_swin_tiny_commercial_trainable.pth \\\n",
    "            results_dir=$RESULTS_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Provide training specification <a class=\"anchor\" id=\"head-4\"></a>\n",
    "\n",
    "We provide specification files to configure the training parameters including:\n",
    "\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * train_data_sources:\n",
    "        * image_dir: the root directory for train images\n",
    "        * json_file: ODVG annotation file\n",
    "        * label_map: category id and category mapping\n",
    "    * val_data_sources: \n",
    "        * image_dir: the root directory for validation images\n",
    "        * json_file: annotation file for validation data. Required to be in COCO json format and the categoy id should be in the range of 0 ~ # of classes - 1\n",
    "    * max_labels: max number of positive + negative labels seen in a single batch. Larger max_labels usually result in better accuracy with longer training time.\n",
    "    * batch_size: batch size for dataloader\n",
    "    * workers: number of workers to do data loading\n",
    "* model: configure the model setting\n",
    "    * pretrained_backbone_path: path to the pretrained backbone model. Only Swin-variants are supported\n",
    "    * num_feature_levels: number of feature levels used from backbone\n",
    "    * dec_layers: number of decoder layers\n",
    "    * enc_layers: number of encoder layers\n",
    "    * num_queries: number of queries for the model\n",
    "    * num_select: number of top-k proposals to select from\n",
    "    * use_dn: flag to enable denoising during training\n",
    "    * dropout_ratio: drop out ratio\n",
    "* train: configure the training hyperparameters\n",
    "    * num_gpus: number of gpus \n",
    "    * num_nodes: number of nodes (num_nodes=1 for single node)\n",
    "    * val_interval: validation interval\n",
    "    * optim:\n",
    "        * lr_backbone: learning rate for backbone\n",
    "        * lr: learning rate for the rest of the model\n",
    "        * lr_steps: learning rate decay step milestone (MultiStep)\n",
    "    * num_epochs: number of epochs\n",
    "    * activation_checkpoint: recompute activations in the backward to save GPU memory. Default is `True`.\n",
    "    * precision: If set to fp16/bf16, the training is run on Automatic Mixed Precision (AMP)\n",
    "    * distributed_strategy: Default is `ddp`. `ddp_sharded` is also supported.\n",
    "\n",
    "Please refer to the TAO documentation about Grounding DINO to get all the parameters that are configurable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:35:58.238279Z",
     "start_time": "2024-09-25T16:35:58.124432Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\r\n",
      "  num_gpus: 1\r\n",
      "  num_nodes: 1\r\n",
      "  validation_interval: 1\r\n",
      "  optim:\r\n",
      "    lr_backbone: 2e-06\r\n",
      "    lr: 2e-5\r\n",
      "    lr_steps: [4]\r\n",
      "    momentum: 0.9\r\n",
      "  num_epochs: 6\r\n",
      "  freeze: [\"backbone\", \"bert\", \"transformer.encoder\", \"input_proj\"]\r\n",
      "  pretrained_model_path: /workspace/tao-experiments/grounding_dino/grounding_dino_vgrounding_dino_swin_tiny_commercial_trainable_v1.0/grounding_dino_swin_tiny_commercial_trainable.pth\r\n",
      "  precision: bf16\r\n",
      "dataset:\r\n",
      "  train_data_sources:\r\n",
      "    - image_dir: /data/HardHatWorkers/raw/train/\r\n",
      "      json_file: /data/odvg/annotations/annotations_without_background_odvg.jsonl\r\n",
      "      label_map: /data/odvg/annotations/annotations_without_background_odvg_labelmap.json\r\n",
      "  val_data_sources:\r\n",
      "    image_dir: /data/HardHatWorkers/raw/valid/\r\n",
      "    json_file: /data/odvg/annotations/annotations_without_background_remapped.json\r\n",
      "  max_labels: 80\r\n",
      "  batch_size: 8\r\n",
      "  workers: 8\r\n",
      "model:\r\n",
      "  backbone: swin_tiny_224_1k\r\n",
      "  num_feature_levels: 4\r\n",
      "  dec_layers: 6\r\n",
      "  enc_layers: 6\r\n",
      "  num_queries: 900\r\n",
      "  dropout_ratio: 0.0\r\n",
      "  dim_feedforward: 2048\r\n",
      "  log_scale: auto\r\n",
      "  class_embed_bias: True\r\n"
     ]
    }
   ],
   "source": [
    "!cat $HOST_SPECS_DIR/train.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run TAO training <a class=\"anchor\" id=\"head-5\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* Evaluation uses COCO metrics. For more info, please refer to: https://cocodataset.org/#detection-eval\n",
    "* We only finetune the decoders and linear layers of Grounding DINO and freeze most other layers. Depending on the size of your dataset, unfreezing other parts of the network can help boost your final mAP.\n",
    "* The training can be completed within several hours on a single gpu with about 20GBs of VRAM. We recommend using more powerful GPUs if your dataset is larger or you want to finetune the larger variant of Grounding DINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, change train.num_gpus in train.yaml or via the command line based on your machine to the desired number of GPUs.\")\n",
    "os.environ[\"NUM_TRAIN_GPUS\"] = \"1\"\n",
    "\n",
    "!tao model grounding_dino train \\\n",
    "           -e $SPECS_DIR/train.yaml \\\n",
    "           train.num_gpus=$NUM_TRAIN_GPUS \\\n",
    "           results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trained checkpoints:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set NUM_EPOCH to the epoch corresponding to any saved checkpoint\n",
    "# %env NUM_EPOCH=006\n",
    "\n",
    "# Get the name of the checkpoint corresponding to your set epoch\n",
    "# tmp=!ls $HOST_RESULTS_DIR/train/*.pth | grep epoch_$NUM_EPOCH\n",
    "# %env CHECKPOINT={tmp[0]}\n",
    "\n",
    "# Or get the latest checkpoint\n",
    "os.environ[\"CHECKPOINT\"] = os.path.join(os.getenv(\"HOST_RESULTS_DIR\"), \"train/gdino_model_latest.pth\")\n",
    "\n",
    "print('Rename a trained model: ')\n",
    "print('---------------------')\n",
    "!cp $CHECKPOINT $HOST_RESULTS_DIR/train/grounding_dino_model.pth\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train/grounding_dino_model.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate a trained model <a class=\"anchor\" id=\"head-6\"></a>\n",
    "\n",
    "In this section, we run the `evaluate` tool to evaluate the trained model and produce the mAP metric.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration.\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * test_data_sources:\n",
    "        * image_dir: the root directory for evaluatation images    \n",
    "        * json_file: Required to be in COCO json format and the categoy id should be in the range of 0 ~ # of classes - 1\n",
    "    * batch_size\n",
    "    * workers\n",
    "* evaluate:\n",
    "    * num_gpus: number of gpus\n",
    "    * conf_threshold: a threshold for confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on TAO model\n",
    "!tao model grounding_dino evaluate \\\n",
    "            -e $SPECS_DIR/evaluate.yaml \\\n",
    "            evaluate.checkpoint=$RESULTS_DIR/train/grounding_dino_model.pth \\\n",
    "            results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Inferences <a class=\"anchor\" id=\"head-7\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models and visualize the results. The `inference` tool produces annotated image outputs and txt files that contain prediction information.\n",
    "\n",
    "We provide inference.yaml specification files to configure the inference parameters including:\n",
    "\n",
    "* model: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration\n",
    "* dataset: configure the dataset and augmentation methods\n",
    "    * infer_data_sources:\n",
    "        * image_dir: the list of directories for inference images\n",
    "        * captions: list of phrases to run inference on. E.g. [\"person\", \"black cat\"]\n",
    "    * batch_size\n",
    "    * workers\n",
    "* inference\n",
    "    * conf_threshold: the confidence score threshold\n",
    "    * color_map: the color mapping for each phrase. The predicted bbox will be drawn with mapped color for each phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model grounding_dino inference \\\n",
    "        -e $SPECS_DIR/infer.yaml \\\n",
    "        inference.checkpoint=$RESULTS_DIR/train/grounding_dino_model.pth \\\n",
    "        results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg']\n",
    "\n",
    "def visualize_images(output_path, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"inference\", \"images_annotated\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX model\n",
    "!tao model grounding_dino export \\\n",
    "           -e $SPECS_DIR/export.yaml \\\n",
    "           export.checkpoint=$RESULTS_DIR/train/grounding_dino_model.pth \\\n",
    "           export.onnx_file=$RESULTS_DIR/export/grounding_dino_model.onnx \\\n",
    "           results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TensorRT engine using tao deploy\n",
    "!tao deploy grounding_dino gen_trt_engine -e $SPECS_DIR/gen_trt_engine.yaml \\\n",
    "                               gen_trt_engine.onnx_file=$RESULTS_DIR/export/grounding_dino_model.onnx \\\n",
    "                               gen_trt_engine.trt_engine=$RESULTS_DIR/gen_trt_engine/grounding_dino_model.engine \\\n",
    "                               results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with generated TensorRT engine\n",
    "!tao deploy grounding_dino evaluate -e $SPECS_DIR/evaluate.yaml \\\n",
    "                              evaluate.trt_engine=$RESULTS_DIR/gen_trt_engine/grounding_dino_model.engine \\\n",
    "                              results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with generated TensorRT engine\n",
    "!tao deploy grounding_dino inference -e $SPECS_DIR/infer.yaml \\\n",
    "                              inference.trt_engine=$RESULTS_DIR/gen_trt_engine/grounding_dino_model.engine \\\n",
    "                              results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"trt_inference\", \"images_annotated\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
