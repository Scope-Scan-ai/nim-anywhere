train:
  num_gpus: 2
  num_nodes: 1
  validation_interval: 1
  optim:
    lr_backbone: 2e-06
    lr: 2e-5
    lr_steps: [4]
    momentum: 0.9
  num_epochs: 6
  freeze: ["backbone", "bert", "transformer.encoder", "input_proj"]
  pretrained_model_path: /workspace/tao-experiments/grounding_dino/grounding_dino_vgrounding_dino_swin_tiny_commercial_trainable_v1.0/grounding_dino_swin_tiny_commercial_trainable.pth
  precision: bf16
dataset:
  train_data_sources:
    - image_dir: /data/HardHatWorkers/raw/train/
      json_file: /data/odvg/annotations/annotations_without_background_odvg.jsonl
      label_map: /data/odvg/annotations/annotations_without_background_odvg_labelmap.json
  val_data_sources:
    image_dir: /data/HardHatWorkers/raw/valid/
    json_file: /data/odvg/annotations/annotations_without_background_remapped.json
  max_labels: 80
  batch_size: 8
  workers: 8
model:
  backbone: swin_tiny_224_1k
  num_feature_levels: 4
  dec_layers: 6
  enc_layers: 6
  num_queries: 900
  dropout_ratio: 0.0
  dim_feedforward: 2048
  log_scale: auto
  class_embed_bias: True
